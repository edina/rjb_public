package EPrints::Plugin::Import::Elife;

=pod

=for Pod2Wiki

=head1 NAME

B<EPrints::Plugin::Import::Elife> - The importer specifically for the ELife
data feed.

=head1 DESCRIPTION

The model used for this importer is that Elife FTP to us the same data that
they FTP to HighWire. A shell script checks the FTP directory and assumes
that each subdirectory is a record to be imported. It then calls this script
on each of those subdirectories.

The name of the subdirectory is the article id (Elife id). In this directory
are a number of files all of which are zip files.

We unpack the files and look for an XML file which may be just the metadata
or may be the metadata and data in XML.

We take the <front> material and extract data from it to create the new
eprints record and attach all the rest of the files to the new eprints record.

If we get a zip file which contains a directory, we leave it as a zip file. If
we get a zip file that looks correupt we bomb out with an error message unless
the 'force' parameter is set in which case we just add the file as it is.

=cut

# The gets passed the 'filename' from the import command which in this case
# is the directory name.

# Arguments
#
# According to the docs the import script takes an optional parameter
#      --argument K=V
# which gets passed into the plugin. Except it doesn't. So currently
# things like force, verbose etc can only be set here.
#

# Return Codes
#
# The import script does not pass on any return codes generated by
# the plugin. So there is currently no way for a shell script calling
# 'import' to tell if the plugin failed or not. This is needed, for exampe,
# to know if you should delete files because the import worked.
# To get round it, this plugin writes the result code to /tmp
# which can then be picked up by the shell script.


use parent EPrints::Plugin::EdinaImporter;


use strict;
use 5.010;

use Encode;
use Data::Dumper;
use Archive::Zip qw( :ERROR_CODES );
use IPC::Run qw(run timeout);
use XML::LibXML;
use File::Slurp;
use Sys::Hostname;

# globals 
my $indent = 0;
my $ziperror = 0;

my $verbose = 0;
my $force = 0;


#
# map of article types in elife data to article types in eprints
#
my %typemap = (
    'research-article' => 'article',
    'editorial' => 'monograph',
    'discussion' => 'monograph',
    'article-commentary' => 'monograph',
    'correction' => 'correction');


sub typeOf {
    my ($plugin, $atype) = @_;
    return $typemap{$atype} or 'article';
}


=pod

=head1 SUBROUTINES/METHODS

=over 4

=item new

This is the instanciator for the importer-class

Basically, it creates a generic C<EPrints::Plugin::Import> object, but
defines the I<provenance> of the import as 'ELife'

=cut

sub new
{
        my( $class, %params ) = @_;

        my $self = $class->SUPER::new( %params );

        $self->{noise} = $verbose;
        $self->{name} = 'Elife';
        $self->{visible} = 'all';
        $self->{produce} = [ 'list/eprint', 'dataobj/eprint' ];

        return $self;
}



#
# Import calls input_file on the filename passed in but in this case
# it is actually a directory containing a set of zip files representing
# one eprints record.
#
# Done in five stages:
#
# - unpack all the files from the zip archives
#
# - extract the XML from the metadata file (which may also contain the document in XML).
#
# - extract the info we want out of the XML and create a epdata (hash) structure.
#
# - use the epdata hash to create a new Eprints object.
#
# - associate any documents with the eprints record
#

=pod

=item input_file

The EPrint importer looks for an C<input_file> method, and we have a bespoke
one for this importer

in essence, it unpacks all the files, extracts the metadata file(s), creates
an C<$epdata> hash-ref from the XML (which will have multiple documents
attached), and then creates the record in EPrints.
 

=cut

sub input_file
{
    my( $plugin, %opts ) = @_;
my $startTime = time;

    $plugin->plant_error_code(0);

    importer_log(">inputfile");


    my $session = $plugin->{session};
     
    # figure out the logging for transfers
    my $error_leader = "\n[FTP-DEPOSIT] [Elife] [INTERNAL-ERROR]";
    my $log_dir      = $session->config('archiveroot');
    my $LOGDIR       = "$log_dir/var";
    my $logging      = '|Elife_FTP_Import';
    my $log_file     = hostname . '-transfer_log';

    importer_log("log_dir=$log_dir");
    importer_log("LOGDIR=$LOGDIR");
    importer_log("log_file=$log_file");
    

    eval {
        my $dirname = $opts{filename};
        importer_log("looking in dir $dirname");

        $plugin->{dataset} = $opts{dataset};

        my $files = $plugin->unpack_files($dirname);
        my $xml = $plugin->extract_metadata( $files );
        my $epdata = $plugin->xml_to_epdata( $xml, $dirname );
        my $eprint = $plugin->epdata_to_dataobj( $plugin->{dataset}, $epdata ); 
        if( !defined $eprint ) {
            die("Failed to create DataObj from epdata ($!):\n" . Dumper($epdata));
        }

        # Our counts are about potential customers for broker:
        # $org_count is the number of orgs identified as existing in ORI *AND*
        # have repositories
        # $repos_count is the number of repos associated with the eprint.
        # These can be different as 1 org may have multiple repos.

       # my %uniq_orgs = ();
       # my %uniq_repos = ();
       # my $orgs_count = scalar(keys %uniq_orgs);
       # my $repos_count = scalar(keys %uniq_repos);

       # if($epdata->{broker_orgid} && $epdata->{broker_repoid}) {
       # my @orgs = @{$epdata->{broker_orgid}};
       # importer_log("Found " . scalar(@orgs) . " orgs");
       # my @repos = @{$epdata->{broker_repoid}};
       # importer_log("Found " . scalar(@repos) . " repos");
       # if(scalar(@orgs) != scalar(@repos)) {
       #     importer_log("ERROR: in broker epdata records nOrgs != nRepos and it should be");
       # } else {
       #     foreach (@orgs) {
       #     my $repo = shift(@repos);
       #     if(defined $repo) {
       #         $uniq_orgs{$_} = 1;
       #         $uniq_repos{$repo} = 1;
       #     }
       #     }
       # }
    
       # $orgs_count = scalar(keys %uniq_orgs);
       # $repos_count = scalar(keys %uniq_repos);
       # }
       my ( $orgs_count, $repos_count ) = ( 0, 0 );

       $orgs_count = scalar @{ delete $epdata->{target_orgs} }
          if exists $epdata->{target_orgs};
       $repos_count = scalar @{ $epdata->{broker_orgid} }
          if exists $epdata->{broker_orgid};


        my $nfiles = $plugin->add_files($eprint);

        my $log_string = scalar(CORE::localtime)
        . '|Elife_FTP_Import|'
        . $eprint->get_id
        . "|$orgs_count|$repos_count|"
        . $nfiles;

        open my $LOG, ">>$LOGDIR/$log_file" or
            $plugin->{session}->log("$error_leader Could not open log file $LOGDIR/$log_file");
        print $LOG "$log_string\n";
        importer_log("LOGSTRING=$log_string");
        close $LOG;

        importer_log("New eprintid=" . $eprint->get_id);
        importer_log("<inputfile");
    };





    if($@) {
        print STDERR $@;
        $plugin->plant_error_code(1);
        exit(1); 
    }

    return EPrints::List->new(
                    dataset => $opts{dataset},
                    session => $plugin->{session},
                    ids=>[] );
}




#
# given a set of files, find the file that contains the metadata (*.xml)
# and return the XML from it
# we assume there is only one .xml file
# use File::Slurp to slurp it in and LibXML to parse.
#

sub extract_metadata {
    my( $plugin, $files ) = @_;
    importer_log('>find_metadata_file');

    # find the metadata file
    $plugin->{files} = $files;
    importer_log('found files=');
    loglist(@$files);
    my @possible_metafiles = grep {/.xml$/} @$files;
    my $nPossibles = scalar(@possible_metafiles);
    my $metafilename = $possible_metafiles[0];
    if( $nPossibles <= 0 ) {
        die("Cannot find a metafile (.xml) in [" . join(',',@$files) . "]");
    }
    if($nPossibles > 1) {
        print STDERR "WARNING: got $nPossibles possible meta files - taking $metafilename and going with that\n";
    }

    my $xmlStr = read_file($metafilename);
    my $xml = XML::LibXML->load_xml(string => $xmlStr, recover => 1);
        
    importer_log('<find_metadata_file');
    return $xml;
}




#
# Process the XML to produce an epdata hash from which
# to build the Eprints DataObj.
#
# Preprocess the xml through the ltg script to add in orgids
# BUT can't handle the whole thing - barfs on stuff in the body
# so just pull out the <front> matter and process that.
#

sub xml_to_epdata {
    my( $plugin, $xml, $dirname ) = @_;
    my $epdata = {};
    importer_log(">xml_to_epdata");

    my $xmlSup;    # xml plus ltg added codes
    my $error;

    # extract article type from <article> tag
    my ($articleNode) = $xml->findnodes('article');
    my $articleType = $articleNode->getAttribute('article-type');
    importer_log("articleType=$articleType");
    $epdata->{type} = $plugin->typeOf($articleType);
    importer_log("this converts to " . $epdata->{type});

    if($articleType eq 'correction') {
        print STDERR "Elife $dirname: Document type is correction - leave for now\n";
    }

    # extract the front matter
    my ($front) = $xml->findnodes('article/front');

    # get rid of executive-summary
    my @exec_summary = $front->findnodes('article-meta/abstract[@abstract-type="executive-summary"]');
    importer_log("Number of exec summaries=" . scalar(@exec_summary));
    foreach (@exec_summary) {
        $_->unbindNode();
    }
    #
    # text may have unicode hex codes in it (&#x2020 etc)
    # this sorts that out
    my $frontString = encode('UTF8',$front->toString(1));

    #==================================================================
    #
    # Somehow, the XML needs to be parsed to identify organisations,
    # and to assign ORI ids to those orgs... so we can then find
    # repos for orgs.
    #
    # NOTE
    # We want to identify orgs without repos too, because we want to
    # encourage orgs to create IRs, and thus further the OA movement
    # globally.
    #==================================================================

    # re-parse the supplemented xml
    $xml = XML::LibXML->load_xml(string => $xmlSup);
    
    if ( !defined $xml ) {
        print STDERR "Elife: failed to parse supplemented xml\n";
        $plugin->plant_error_code(1);
        exit(1);
    }

    #
    # now extract the bits we want
    #

    ($front) = $xml->findnodes('front');
    $plugin->fatal($front,'front');

    my ($jmeta) = $front->findnodes('journal-meta');
    $plugin->fatal($jmeta,'journal-meta');

    my $jtitle = $jmeta->findvalue('journal-title-group/journal-title');
    importer_log("JTITLE=$jtitle");
    $epdata->{'publication'} = $jtitle || '';

    my $publisherName = $jmeta->findvalue('publisher/publisher-name');
    importer_log( "PUBLISHER NAME=" . $publisherName);
    $epdata->{'publisher'} = $publisherName || '';

    my $issn = $jmeta->findvalue('issn');
    importer_log( "ISSN=$issn");
    $epdata->{issn} = $issn || '';

    my ($ameta) = $front->findnodes('article-meta');
    $plugin->fatal($ameta,'article-meta');

    my $doi = $ameta->findvalue('article-id[@pub-id-type="doi"]');
    $doi = "http://doi.org/$doi" if($doi);
    importer_log( "DOI=$doi" );
    $epdata->{'doi'} = $doi || '';

    # also add doi as a "related url"
    $epdata->{'related_url_url'} = [ $doi ];

    my $pubid = $ameta->findvalue('article-id[@pub-id-type="publisher-id"]');
    importer_log( "PUBID=$pubid" );
    $epdata->{'id_number'} = $pubid || '';

    my $title = $ameta->findvalue('title-group/article-title');
    importer_log( "TITLE=$title" );
    $epdata->{title} = $title || '';

    my $volume = $ameta->findvalue('volume');
    importer_log( "VOLUME=$volume" );
    $epdata->{volume} = $volume || '';
    
    my ($dateNode) = $ameta->findnodes('history/date[@date-type="accepted"]');
    ($dateNode) = $ameta->findnodes('pub-date') if(!defined($dateNode));
    my $year = $dateNode->findvalue('year');
    my $month = $dateNode->findvalue('month');
    my $day = $dateNode->findvalue('day');
    my $date = "$year/$month/$day";
    importer_log( "DATE=$date" );
    $epdata->{date} = "$year-$month-$day";
    $epdata->{date_type} = "published";

    # [1] means 'stop at the first node that matches rather than fetching all that match
    # so this assumes that the first <p> in the first <abstract> is the one we want
    my $abstract = $ameta->findvalue('abstract[1]/p[1]');
    importer_log( "ABSTRACT=$abstract");
    $epdata->{abstract} = $abstract;


    #
    # go through the <aff section and create maps for the info
    # which we later use for authors
    # In most cases, the aff nodes reside under contrib-group
    # so we have:
    #      contrib-group
    #         - contrib+
    #         - aff+
    # but in some cases where there is only one author we get
    #      contrib-group
    #         - contrib
    #            - aff
    #
    my %aff_to_orgname = ();
    my %aff_to_institution = ();
    my %target_orgs = ();
    my %aff_email = ();

    my @contribGroups = $ameta->findnodes('contrib-group');
    importer_log("Found ".scalar(@contribGroups)." contrib-group nodes");
    my $cg = $contribGroups[0];
    my @affnodes = ();
    @affnodes = $cg->findnodes('aff') if($cg);
    if($cg && scalar(@affnodes) == 0) {
        @affnodes = $cg->findnodes('contrib/aff');
    }
    importer_log("Found ".scalar(@affnodes)." aff nodes");
    foreach (@affnodes) {
        my $affid = $_->getAttribute('id');
        next if !defined($affid);

        my @labelNodes = $_->findnodes('label');

        # remove <label> sections so they when we later do
        # textCotent they don't mess things up
        foreach (@labelNodes) {
            $_->unbindNode();
        }

        my @orgnodes = $_->findnodes('institution/org');
        my $orgnode = scalar(@orgnodes) ? $orgnodes[0] : undef;
        my @orgids = ();
        foreach (@orgnodes) {
            my $orgid = $_->getAttribute('orgid');
            my $poss = $_->getAttribute('poss');

            if($orgid && $poss && $poss eq 'yes') {
                $target_orgs{$orgid} = 1;
            }
        }

        $aff_email{$affid} = '';
        importer_log("Looking for email in aff section");
        my @emailNodes = $_->findnodes('email');
        importer_log("Found ".scalar(@emailNodes)." email nodes");
        if(scalar(@emailNodes)) {
            my $emailAddr = $emailNodes[0]->textContent;
            importer_log("Setting email for $affid to $emailAddr");
            $aff_email{$affid} = $emailAddr;
            $emailNodes[0]->unbindNode();
        }
        $aff_to_institution{$affid} = $_->textContent;
        $aff_to_orgname{$affid} = join ',', map {$_->textContent} @orgnodes;
    }
    
    #
    # we build a set of arrays (creators, institutions, orcirds etc)
    # which need to be matching. So if we don't have a value (e.g. no orcid)
    # we need to put an empty value in to keep them in step.
    #
    my @authors = $ameta->findnodes('contrib-group/contrib[@contrib-type="author"]');
    my @creators = ();
    my @orgname = ();
    my @orcids = ();
    my @institutions = ();
    my @email = ();

    foreach my $auth (@authors) {
        my $surname = $auth->findvalue('name/surname');
        my $forename = $auth->findvalue('name/given-names');
        if($surname and $forename) {    #TODO: too strict? just surname?
        push(@creators, {family => $surname, given => $forename});
        my @affs = $auth->findnodes('xref[@ref-type="aff"]');
            my $affid = scalar(@affs) ? $affs[0]->getAttribute('rid') : undef;
        push(@orgname,defined($affid) ? $aff_to_orgname{$affid} : '');    # TODO: can be more than one institution
            my $orcid = $auth->findvalue('contrib-id[@contrib-id-type="orcid"]') || '';
            push( @orcids, $orcid );
            push( @institutions, defined($affid) ? $aff_to_institution{$affid} : '' );
        my ($correspNode) = $auth->findnodes('xref[@ref-type="corresp"]');
            my $email = '';
            if(defined $correspNode) {
                my $correspId = $correspNode->getAttribute("rid");
                my ($emailNode) = $ameta->findnodes('author-notes/corresp[@id="' . $correspId . '"]/email');
                $email = $emailNode->textContent if($emailNode);
            }
            # if no email in author section, could be one in aff - a bit dodgy
            # because one aff section can be shared by multiple authors but that's
            # how they roll.
            $email = $aff_email{$affid} if(!$email && $affid && $aff_email{$affid});
            importer_log("email=$email");
            if($email && !defined($epdata->{contact_email})) {
                $epdata->{contact_email} = $email;
            }
            push(@email,$email);

        }
    }
    $epdata->{creators_name}  = \@creators;
    $epdata->{creators_orgname} = \@orgname;
    $epdata->{creators_orcid} = \@orcids;
    $epdata->{creators_institution} = \@institutions;
    $epdata->{creators_id}  = \@email;
    importer_log("Email list=" . join(';',@email));

    importer_log("Authors:");
    foreach my $i (0..$#creators) {
        my $creator = $creators[$i];
        importer_log("    $creator->{'family'}, $creator->{'given'} | $orgname[$i] | $orcids[$i]");
    }
    importer_log("---------------------------");

    my @kwdNodes = $ameta->findnodes('kwd-group/kwd');
    my @keywords = ();
    foreach (@kwdNodes) {
        my $kwd = $_->textContent;
        if($kwd ne 'None') {
            push(@keywords,$kwd);
        }
    }
    importer_log( "KEYWORDS=" . join(',',@keywords));
    $epdata->{keywords} = join(',',@keywords) if scalar(@keywords);

    my @award_groups = $ameta->findnodes('funding-group/award-group');
    importer_log("found " . scalar(@award_groups) . " award groups");
    my @grants_agency = ();
    my @grants_grantcode = ();
    foreach (@award_groups) {
        my $agency = $_->findvalue('funding-source/institution-wrap/institution');
        $agency =~ s/<\/?[^>]+>/ /g;   # strip out any tags
        $agency =~ s/ +/ /g;           # and any leftover spaces

        my $gcode = $_->findvalue('award-id') || '';
        importer_log("agency = $agency");
        importer_log("gcode = $gcode");
        push(@grants_agency,$agency);
        push(@grants_grantcode,$gcode);
    }
    importer_log("agency list=" . join(',',@grants_agency));
    importer_log("code list=" . join(',',@grants_grantcode));
    $epdata->{grants_agency} = \@grants_agency;
    $epdata->{grants_grantcode} = \@grants_grantcode;



    $epdata->{provenance} = "elife";
    $epdata->{openaccess} = 'TRUE';
    $epdata->{refereed} = 'TRUE';
    $epdata->{ispublished} = 'pub';
    
    # Having built a list of orgids, we need to prod OA-RJ for repo
    # data, so we can start populating the Broker part of the record
    # we also need to de-duplcate the list!!
    my @target_orgs = grep { $_ ne "" } keys %target_orgs; # all non-empty keys!
    $epdata->{target_orgs} = [] unless exists $epdata->{target_orgs};
    push @{ $epdata->{target_orgs} }, @target_orgs;
    $plugin->get_repo_list($epdata);

    if ( $epdata->{issn} and not $epdata->{publisher} ) {
        _get_publisher_from_romeo( $plugin, $epdata );
    }

    importer_log("<xml_to_epdata");
    return $epdata;
}



sub importer_log {
    my ($mess) = @_;
    return if(!$verbose);

    if($mess =~ /^</) {
        $indent--;
        $indent = 0 if($indent < 0);
    }
    
    print STDERR '    ' x $indent;
    print STDERR "$mess\n";
    if($mess =~ /^>/) {
        $indent++;
    }
}

sub loglist {
    return if(!$verbose);
    print STDERR '    ' x $indent;
    print STDERR "(\n";
    foreach (@_) {
        print STDERR '    ' x ($indent + 1);
        print STDERR "$_\n";
    }
    print STDERR '    ' x $indent;
    print STDERR ")\n";
}

sub startOf {
    my ($s,$n) = @_;
    if(length($s) < $n) {
        return $s;
    } else {
        return substr($s,0,$n);
    }
}


sub fatal {
    my ($plugin,$val,$where) = @_;
    if(!defined($val)) {
        print STDERR "FATAL ERROR: couldnae find section $where - bailing!\n" if(!defined($val));
        $plugin->plant_error_code(1);
        exit(1);
    }
}


sub unpack_files {
    my ($plugin,$dirname) = @_;
    importer_log(">unpack files: $dirname");
    my $dh;
    if( !opendir($dh, $dirname) )
    {
        $plugin->error("Could not read directory $dirname for import: $!");
        return undef;
    }

    # default handler just bombs. Want something a little more subtle.
    Archive::Zip::setErrorHandler( \&myErrorHandler );

    my @files = ();

    # only want to look at zip files starting with elife
    my @zipfiles = map {$dirname . '/' . $_} grep {/^elife.*\.zip$/} readdir $dh;

    foreach my $zfile (@zipfiles) {
        my $isDir = 0;
        importer_log("found file $zfile");
        importer_log("extracting $zfile to $dirname");

        my $archive = Archive::Zip->new($zfile);

        # if there's a corrupt zip file we bail out and give
        # a warning unless force is set in which case we just add it
        if($ziperror) {
            if(!$force) {
                print STDERR "Zip file looks corrupt: $zfile\n";
                $plugin->error("Zip file looks corrupt: $zfile\n");
                $plugin->plant_error_code(1);
                exit(1);
            }
            push(@files,$zfile);
            importer_log("Cannot unzip so just add as is");
            $ziperror = 0;
            next;
        }

        # if zip file contains dirs, leave as a zip file
        # otherwise unpack it.
        my @members = $archive->members();
        foreach my $member (@members) {
            if( ref($member) =~ /Directory/) {
                $isDir = 1;
                last;
            }
        }
        if($isDir) {
            importer_log("contains a directory so leave as a zip");
            push(@files,$zfile);
        } else {
            foreach my $member (@members) {
                $dirname .= '/' if($dirname !~ /\/$/);
                my $name = $dirname . $member->fileName();
                importer_log("NAME=$name");
                my $ret = $archive->extractMember($member, $name);
                importer_log("Zip Extract returned $ret");
                push(@files,$name);
                importer_log("Extracted: $name");
            }
        }
    }

    closedir($dh); 
    importer_log("<unpack files: $dirname");
    return \@files;
}



sub myErrorHandler {
    importer_log("ERROR:");
    loglist(@_);
    $ziperror = 1;
}


sub plant_error_code {
    my ($plugin,$code) = @_;
    open(my $ofh, '>', '/tmp/elifecode');
    print $ofh "$code\n";
    close($ofh);
}

1;

=pod

=back

=head1 DEPENDENCIES

This package is used within EPrints.

There is a dependency on some tool to identify organisations within
text strings, and assign ORI org_ids to them.


It is also dependent on the ORI service to map ORI org_ids to appropriate
repositories

=head1 SEE ALSO

EPrints (http://epriints.org)

=head1 AUTHOR

Ian Stuart <Ian.Stuart@ed.ac.uk>
Ray Carrick <Ray.Carrick@ed.ac.uk>

2014-2015

=head1 LICENSE

EPrints is GNU licensed, so this distributed code is also GNU licensed

=cut

